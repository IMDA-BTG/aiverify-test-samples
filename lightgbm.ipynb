{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13aed560-3c0f-4d1f-bd54-3a085ccad7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './models/lightgbm/3.3.5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/lightgbm/\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lightgbm\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(save_dir)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './models/lightgbm/3.3.5'"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "import pkgutil\n",
    "import os\n",
    "import importlib, inspect\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "save_dir = \"./models/lightgbm/{0}\".format(lightgbm.__version__)\n",
    "# print(save_dir)\n",
    "os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0811af3d-c29b-47fb-b8da-23f1ef975b45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset',\n",
       " 'Booster',\n",
       " 'CVBooster',\n",
       " 'Sequence',\n",
       " 'register_logger',\n",
       " 'train',\n",
       " 'cv',\n",
       " 'LGBMModel',\n",
       " 'LGBMRegressor',\n",
       " 'LGBMClassifier',\n",
       " 'LGBMRanker',\n",
       " 'DaskLGBMRegressor',\n",
       " 'DaskLGBMClassifier',\n",
       " 'DaskLGBMRanker',\n",
       " 'log_evaluation',\n",
       " 'print_evaluation',\n",
       " 'record_evaluation',\n",
       " 'reset_parameter',\n",
       " 'early_stopping',\n",
       " 'plot_importance',\n",
       " 'plot_split_value_histogram',\n",
       " 'plot_metric',\n",
       " 'plot_tree',\n",
       " 'create_tree_digraph']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm.__all__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898eb1f-31aa-47f8-88cc-0bf7bb9707e4",
   "metadata": {},
   "source": [
    "# Generate lightgbm model samples for rocket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a641df-6170-48fc-90b8-8ef01fdd474c",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc16ae1-ceb7-47b9-9910-9d4dc3262415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compas_training = pickle.load(open(\"./data/pickle_pandas_tabular_compas_training.sav\", \"rb\"))\n",
    "compas_testing = pickle.load(open(\"./data/pickle_pandas_tabular_compas_testing.sav\", \"rb\"))\n",
    "\n",
    "X_train = compas_training.drop(\"two_year_recid\", axis=1)\n",
    "y_train = compas_training[[\"two_year_recid\"]]\n",
    "X_test = compas_testing.drop(\"two_year_recid\", axis=1)\n",
    "y_test = compas_testing[[\"two_year_recid\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5737e1-82d5-4ef1-8c10-262e42f0e8c5",
   "metadata": {},
   "source": [
    "### Booster - binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6247c87a-ec61-434e-84cb-0a65e15d1a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2244, number of negative: 2693\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 4937, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.454527 -> initscore=-0.182396\n",
      "[LightGBM] [Info] Start training from score -0.182396\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(X_train, label=y_train)\n",
    "dtest = lightgbm.Dataset(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'binary'\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/binary_classification_compas_binary_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044aaf8-953f-4ae9-b876-7588af7953a7",
   "metadata": {},
   "source": [
    "### Booster - cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be7fbce-1560-4221-910e-e53b79ccd4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Unknown objective type name: cross-entropy\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Unknown objective type name: cross-entropy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m param \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m31\u001b[39m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross-entropy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      6\u001b[0m num_round \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 7\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mlightgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_round\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(bst)))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/lightgbm/basic.py:2610\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2608\u001b[0m params_str \u001b[38;5;241m=\u001b[39m param_dict_to_str(params)\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m-> 2610\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterCreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;66;03m# save reference to data\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m train_set\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Unknown objective type name: cross-entropy"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'cross-entropy'\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/binary_classification_compas_crossentropy_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365279b2-a70a-425a-ae42-96cc0aa2cedd",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c66e6ca-17ab-49f5-ae4b-3e74a517c272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "score: 0.6647773279352227\n"
     ]
    }
   ],
   "source": [
    "classifier = lightgbm.LGBMClassifier().fit(X_train, y_train)  # train using booster class\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"type: {0}\".format(type(classifier)))\n",
    "print(\"score: {0}\".format(classifier.score(X_test, y_test)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.sklearn.LGBMClassifier\\n\")\n",
    "pickle.dump(classifier, open(\"{0}/binary_classification_compas_lightgbm.sklearn.LGBMClassifier.sav\".format(save_dir), \"wb+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed19b4-f47b-4cb7-9e84-a274d6be3adf",
   "metadata": {},
   "source": [
    "### DaskLGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86d3bd1-1d4b-445e-b1a8-132fe9736826",
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "dask, pandas and scikit-learn are required for lightgbm.dask",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mlightgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDaskLGBMClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# train using booster class\u001b[39;00m\n\u001b[1;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(classifier)))\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/lightgbm/dask.py:1177\u001b[0m, in \u001b[0;36mDaskLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m early_stopping_rounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_rounds is not currently supported in lightgbm.dask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lgb_dask_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLGBMClassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/lightgbm/dask.py:1039\u001b[0m, in \u001b[0;36m_DaskLGBMModel._lgb_dask_fit\u001b[0;34m(self, model_factory, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lgb_dask_fit\u001b[39m(\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     model_factory: Type[LGBMModel],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   1037\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_DaskLGBMModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m((DASK_INSTALLED, PANDAS_INSTALLED, SKLEARN_INSTALLED)):\n\u001b[0;32m-> 1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdask, pandas and scikit-learn are required for lightgbm.dask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stopping_rounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_rounds is not currently supported in lightgbm.dask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mLightGBMError\u001b[0m: dask, pandas and scikit-learn are required for lightgbm.dask"
     ]
    }
   ],
   "source": [
    "import dask, pandas\n",
    "classifier = lightgbm.DaskLGBMClassifier().fit(X_train, y_train)  # train using booster class\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"type: {0}\".format(type(classifier)))\n",
    "print(\"score: {0}\".format(classifier.score(X_test, y_test)))\n",
    "\n",
    "# with open(\"algorithm.txt\", \"a+\") as f:\n",
    "#     f.write(\"lightgbm.sklearn.LGBMClassifier\\n\")\n",
    "# pickle.dump(classifier, open(\"./models/binary_classification_compas_lightgbm.sklearn.LGBMClassifier.sav\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c893a28-1d33-4c9e-838a-08ebc20f6ca9",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693e2dbe-0c92-47f0-900a-8abc1c15da62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loan_training = pickle.load(open(\"./data/pickle_pandas_tabular_loan_training.sav\", \"rb\")).sample(5000)\n",
    "loan_testing = pickle.load(open(\"./data/pickle_pandas_tabular_loan_testing.sav\", \"rb\")).sample(2000)\n",
    "\n",
    "multiclass_X_train = loan_training.drop(\"Interest_Rate\", axis=1)\n",
    "multiclass_y_train = loan_training[[\"Interest_Rate\"]]\n",
    "multiclass_X_test = loan_testing.drop(\"Interest_Rate\", axis=1)\n",
    "multiclass_y_test = loan_testing[[\"Interest_Rate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ee8fa-2c3f-4b3a-b049-100d0f895b34",
   "metadata": {},
   "source": [
    "### Booster - softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252d9f66-b9a7-4d23-9a9d-1c62fb525466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 5000, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.589635\n",
      "[LightGBM] [Info] Start training from score -0.853786\n",
      "[LightGBM] [Info] Start training from score -0.993712\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(multiclass_X_train, label=multiclass_y_train)\n",
    "dtest = lightgbm.Dataset(multiclass_X_test, label=multiclass_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'softmax',\n",
    "    'num_class': 3,\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/multiclass_classification_loan_softmax_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e8b3f-b079-4bea-b28a-921690ed7dd9",
   "metadata": {},
   "source": [
    "### Booster - ova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee55ad0a-dcdd-461d-b94e-13d1cd142aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1020, number of negative: 3980\n",
      "[LightGBM] [Info] Number of positive: 2129, number of negative: 2871\n",
      "[LightGBM] [Info] Number of positive: 1851, number of negative: 3149\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 980\n",
      "[LightGBM] [Info] Number of data points in the train set: 5000, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.204000 -> initscore=-1.361479\n",
      "[LightGBM] [Info] Start training from score -1.361479\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.425800 -> initscore=-0.299008\n",
      "[LightGBM] [Info] Start training from score -0.299008\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.370200 -> initscore=-0.531359\n",
      "[LightGBM] [Info] Start training from score -0.531359\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(multiclass_X_train, label=multiclass_y_train)\n",
    "dtest = lightgbm.Dataset(multiclass_X_test, label=multiclass_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'ova',\n",
    "    'num_class': 3,\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/multiclass_classification_loan_onevsall_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a869f-e6bb-4e6f-a738-4a44cbcc3285",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29254ce9-f501-4b2a-abd6-0a3227994381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "score: 0.5035\n"
     ]
    }
   ],
   "source": [
    "classifier = lightgbm.LGBMClassifier().fit(multiclass_X_train, multiclass_y_train)  # train using booster class\n",
    "predictions = classifier.predict(multiclass_X_test)\n",
    "print(\"type: {0}\".format(type(classifier)))\n",
    "print(\"score: {0}\".format(classifier.score(multiclass_X_test, multiclass_y_test)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.sklearn.LGBMClassifier\\n\")\n",
    "pickle.dump(classifier, open(\"{0}/multiclass_classification_loan_lightgbm.sklearn.LGBMClassifier.sav\".format(save_dir), \"wb+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331adb58-2081-42ec-887d-2732c8876590",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38473dce-e354-4919-be93-949d11714ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "insurance_training = pickle.load(open(\"./data/pickle_pandas_tabular_insurance_training.sav\", \"rb\"))\n",
    "insurance_testing = pickle.load(open(\"./data/pickle_pandas_tabular_insurance_testing.sav\", \"rb\"))\n",
    "insurance_testing.describe()\n",
    "\n",
    "regression_X_train = insurance_training.drop(\"charges\", axis=1)\n",
    "regression_y_train = insurance_training[[\"charges\"]]\n",
    "regression_X_test = insurance_testing.drop(\"charges\", axis=1)\n",
    "regression_y_test = insurance_testing[[\"charges\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca427ba-bf96-4ea4-a2cd-41ea907119dc",
   "metadata": {},
   "source": [
    "### Booster - regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a14841f-edf3-41f5-aa80-ba15567eca04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 13296.106303\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'regression',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_mse_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef33f0-8171-4aad-a0b8-2d852b91200a",
   "metadata": {},
   "source": [
    "### Booster - regression_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7117c118-d458-4611-9657-233d4ea3b082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 9301.893555\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'regression_l1',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_mae_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776da96-e944-4833-91e1-a71c5198d6a1",
   "metadata": {},
   "source": [
    "### Booster - Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f8bb3a-b966-437b-bf9d-88b2245a8936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 13296.106303\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'huber',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_huber_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90d110-3abd-479e-af36-21617d37c650",
   "metadata": {},
   "source": [
    "### Booster - Fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "704720da-a263-4fe5-a058-37e337f1d71e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 13296.106303\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'fair',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_fair_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d20670-bcd8-4928-8736-2b97bcae8139",
   "metadata": {},
   "source": [
    "### Booster - Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2240fca5-3473-4da6-80b8-b44a8dce355e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 9.495227\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'poisson',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_poisson_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518fcf8-a0d1-4b0f-a320-417dcd21b99d",
   "metadata": {},
   "source": [
    "### Booster - Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd40e48-e00a-40b5-b1d9-b1a7b1f9a2df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 35069.371094\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'quantile',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_quantile_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814af-8bf9-4225-a6b8-7c41039c8d7f",
   "metadata": {},
   "source": [
    "### Booster - MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c54fb5c-e65f-4854-b0d7-059484eb56b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 3756.621582\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'mape',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{0}/regression_insurance_mape_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c70a7-dccf-46bd-9dfd-2c23134a3818",
   "metadata": {},
   "source": [
    "### Booster - Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19dd550f-0b09-429e-8d0e-8171098d961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 9.495227\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'gamma',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"{/regression_insurance_gamma_lightgbm.basic.Booster.sav\".format(save_dir), \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e34d7-f64c-4d8e-8e6b-1f58cb3f3451",
   "metadata": {},
   "source": [
    "### Booster - Tweedie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d218d7a-15ae-420a-9c6b-14a9a8bc2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1070, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 9.495227\n",
      "type: <class 'lightgbm.basic.Booster'>\n"
     ]
    }
   ],
   "source": [
    "dtrain = lightgbm.Dataset(regression_X_train, label=regression_y_train)\n",
    "dtest = lightgbm.Dataset(regression_X_test, label=regression_y_test)\n",
    "\n",
    "param = {\n",
    "    'num_leaves': 31, \n",
    "    'objective': 'tweedie',\n",
    "}\n",
    "\n",
    "num_round = 50\n",
    "bst = lightgbm.train(param, dtrain, num_round)\n",
    "print(\"type: {0}\".format(type(bst)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.basic.Booster\\n\")\n",
    "\n",
    "pickle.dump(bst, open(\"./models/regression_insurance_tweedie_lightgbm.basic.Booster.sav\", \"wb+\"))\n",
    "# bst.save_model(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de68ebc-4c6a-4a0f-a3b2-c9356b9a7130",
   "metadata": {},
   "source": [
    "### LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f07cc4-5bc8-47ee-8a85-398dc4eb921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'lightgbm.sklearn.LGBMRegressor'>\n",
      "score: 0.800133848542777\n"
     ]
    }
   ],
   "source": [
    "regressor = lightgbm.LGBMRegressor().fit(regression_X_train, regression_y_train)  # train using booster class\n",
    "predictions = regressor.predict(regression_X_test)\n",
    "print(\"type: {0}\".format(type(regressor)))\n",
    "print(\"score: {0}\".format(regressor.score(regression_X_test, regression_y_test)))\n",
    "\n",
    "with open(\"algorithm.txt\", \"a+\") as f:\n",
    "    f.write(\"lightgbm.sklearn.LGBMRegressor\\n\")\n",
    "pickle.dump(regressor, open(\"./models/regression_insurance_lightgbm.sklearn.LGBMRegressor.sav\", \"wb+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
